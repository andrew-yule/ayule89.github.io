[
  {
    "objectID": "SMU_DDS_CaseStudy2.html",
    "href": "SMU_DDS_CaseStudy2.html",
    "title": "Employee Attrition",
    "section": "",
    "text": "A link to my youtube video discussing this analysis can be found here"
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#introduction",
    "href": "SMU_DDS_CaseStudy2.html#introduction",
    "title": "Employee Attrition",
    "section": "Introduction",
    "text": "Introduction\nThe specific goals of this analysis are to:\n\nIdentify the top three factors that contribute to attrition.\nLearn about any job role specific trends that may exist in the data set (e.g., “Data Scientists have the highest job satisfaction”).\nProvide any other interesting trends and observations from the analysis.\nBuild models to predict attrition and salaries and then utilizing those models on a blind data set."
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#executive-summary",
    "href": "SMU_DDS_CaseStudy2.html#executive-summary",
    "title": "Employee Attrition",
    "section": "Executive Summary",
    "text": "Executive Summary\nUpon completing this analysis, it was learned that the top 3 factors contributing to employee attrition were:\n\nJob Role\n\nStock Option Level\n\nOver Time\n\nThis information was obtained by determining the Pseudo-RSquared parameter for each variable in the data set. The Pseudo-RSquared parameter is a comparison of the likelihood for an outcome given the data as compared to a reference or null model. In this case the null model was the breakdown of attrition across the entire data set, which was roughly 16%.\nNext, a naive-bayes model was constructed using the top 16 variables as ranked by the Pseudo-RSquared. This allowed for a model that achieved an accuracy of 80%, a sensitivity of 84%, and a specificity of 61%.\nFinally, a linear regression model was used with Age, Job Role, Years at Company, and Stock Option variables to achieve an RSquared of 0.85 and a root mean squared error of $1,850."
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#libraries-and-data-loading",
    "href": "SMU_DDS_CaseStudy2.html#libraries-and-data-loading",
    "title": "Employee Attrition",
    "section": "Libraries and Data Loading",
    "text": "Libraries and Data Loading\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(reshape2)\nlibrary(class)\nlibrary(caret)\nlibrary(e1071)\n\ntheme_set(theme_bw())\n\n\nemployeeData = read_csv(\"https://raw.githubusercontent.com/ayule89/SMU_DDS_CaseStudy2/main/Data/CaseStudy2-data.csv\")"
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#identifying-the-top-three-factors-that-contribute-to-attrition",
    "href": "SMU_DDS_CaseStudy2.html#identifying-the-top-three-factors-that-contribute-to-attrition",
    "title": "Employee Attrition",
    "section": "1. Identifying the top three factors that contribute to attrition",
    "text": "1. Identifying the top three factors that contribute to attrition\nHow many of the 870 employees in the data actually left? There were 140 out of 870 employees (16%) that left.\n\nemployeeData |> count(Attrition)\n\n# A tibble: 2 × 2\n  Attrition     n\n  <chr>     <int>\n1 No          730\n2 Yes         140\n\n\nThere are a number of variables that could attribute to employee attrition. We will first visually screen each of the variables to look for potential leading indicators. The full list of variables includes:\n\ncolnames(employeeData)\n\n [1] \"ID\"                       \"Age\"                     \n [3] \"Attrition\"                \"BusinessTravel\"          \n [5] \"DailyRate\"                \"Department\"              \n [7] \"DistanceFromHome\"         \"Education\"               \n [9] \"EducationField\"           \"EmployeeCount\"           \n[11] \"EmployeeNumber\"           \"EnvironmentSatisfaction\" \n[13] \"Gender\"                   \"HourlyRate\"              \n[15] \"JobInvolvement\"           \"JobLevel\"                \n[17] \"JobRole\"                  \"JobSatisfaction\"         \n[19] \"MaritalStatus\"            \"MonthlyIncome\"           \n[21] \"MonthlyRate\"              \"NumCompaniesWorked\"      \n[23] \"Over18\"                   \"OverTime\"                \n[25] \"PercentSalaryHike\"        \"PerformanceRating\"       \n[27] \"RelationshipSatisfaction\" \"StandardHours\"           \n[29] \"StockOptionLevel\"         \"TotalWorkingYears\"       \n[31] \"TrainingTimesLastYear\"    \"WorkLifeBalance\"         \n[33] \"YearsAtCompany\"           \"YearsInCurrentRole\"      \n[35] \"YearsSinceLastPromotion\"  \"YearsWithCurrManager\"    \n\n\nBefore investigating attrition across each variable, some of the numerical scales will need to put into various bins to make analysis easier. In order to determine appropriate bins, we’ll first plot histograms of the numerical variables, which include: Age, DailyRate, DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager.\n\nemployeeData |>\n  select(Age, DailyRate, DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate, TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager) |>\n  pivot_longer(cols = everything(), names_to = \"column\", values_to = \"value\") |>\n  ggplot(aes(x = value)) +\n  geom_histogram() +\n  facet_wrap(~column, scales = 'free', ncol = 3)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBased on visually inspecting the histograms, the following bins for various numerical variables will be used below. In addition, we’ll change the Attrition values from strings to doubles with Yes/No being 1/0 respectively.\n\nAge (5 years)\n\nDaily rate (400 dollars)\n\nDistance from home (5 miles)\n\nHourly rate (20 dollars)\n\nMonthly income (5,000 dollars)\n\nMonthly rate (5,000 dollars)\nTotal working years (5 years)\n\nYears at company (5 years)\n\nYears in current role (5 years)\n\nYears since last promotion (5 years)\n\nYears under current manager (5 years)\n\n\nemployeeDataBinned = employeeData |>\n  mutate(\n    Age = round(Age/5)*5, \n    DailyRate = round(DailyRate/400)*400,\n    DistanceFromHome = round(DistanceFromHome/5)*5,\n    HourlyRate = round(HourlyRate/20)*20,\n    MonthlyIncome = round(MonthlyIncome/5000)*5000,\n    MonthlyRate = round(MonthlyRate/5000)*5000,\n    TotalWorkingYears = round(TotalWorkingYears/5)*5,\n    YearsAtCompany = round(YearsAtCompany/5)*5,\n    YearsInCurrentRole = round(YearsInCurrentRole/5)*5,\n    YearsSinceLastPromotion = round(YearsSinceLastPromotion/5)*5,\n    YearsWithCurrManager = round(YearsWithCurrManager/5)*5)\n\nNow we’ll go through and create plots of each variable showing the breakdown between attrition to visually look for any strong relationships\n\n# Function to create a plot showing the breakdown in attrition for a given variable across it's possible values\nfindAttritionBreakdown = function(df, colName) {\n  df |>\n    group_by(!!sym(colName)) |>\n    count(Attrition) |>\n    ungroup() |>\n    ggplot(aes(x = !!sym(colName), y = n, fill = Attrition)) +\n    labs(title = colName, x = NULL, y = \"Attrition Fraction\") +\n    geom_col(position = \"fill\") +\n    coord_cartesian(ylim = c(0, 0.55))\n}\n# Test the function\n#findAttritionBreakdown(employeeDataBinned, \"DistanceFromHome\")\n\ncols = colnames(employeeData)\n# Drop columns that we don't want to include in the analysis (ID, Attrition, EmployeeCount, EmployeeNumber, Over18, StandardHours) \ncols = cols[c(-1, -3, -10, -11, -23, -28)]\n\n# Create a list of plots across all of the columns using the map function from the purr package\nplots = map(cols, function(col) findAttritionBreakdown(employeeDataBinned, col))\n# Display plots on one page\ngrid.arrange(grobs = plots, ncol = 3)\n\n\n\n\nBased on examining the plots above, the following key variables were identified as likely having higher impacts on employee attrition:\n\nAge\n\nBusinessTravel\n\nDistanceFromHome\n\nEducation\n\nEnvironmentSatisfaction\n\nJobInvolvment\n\nJobRole\n\nJobSatisfaction\n\nMaritalStatus\n\nMonthlyIncome\n\nNumCompaniesWorked\n\nOverTime\n\nStockOptionLevel\n\nTotalWorkingYears\n\nWorkLifeBalance\n\nYearsAtCompany\n\nYearsWithCurrManager\n\nIn order to determine the top 3 factors quantitatively, we will calculate the Pseudo-R2 value for each predictor. This is based on a similar analysis that can be found here.\nThe main idea is that we first start with a Null model, or a baseline model, that can be used to determine the likelihood of attrition. Once that is established, then additional factors can be evaluated against this Null model using their likelihood to look for improvements or increases in the likelihood. In this case, because the likelihoods can be quite large, loglikelihood values will be used.\nThe Null model in this case will simply be the LogLikelihood of attrition in the entire data set or -383.8.\n\ncalcLogLikelihoodData = function(data){\n  no = select(filter(data, Attrition == 0), n)\n  yes = select(filter(data, Attrition == 1), n)\n  if(dim(yes)[1] == 0 || dim(no)[1] == 0) {\n    return(tibble(n = c(0)))\n  }\n  else{\n    p = yes/(yes + no)\n    log(p^yes * (1-p)^no)  \n  }\n}\n\nemployeeDataBinnedWithQuantitativeAttrition = employeeDataBinned |>\n  mutate(Attrition = ifelse(Attrition == \"Yes\", 1, 0))\n\nnullModel = calcLogLikelihoodData(count(employeeDataBinnedWithQuantitativeAttrition, Attrition))\n\ncalcPseudoRSquared = function(var){\n  employeeDataBinnedWithQuantitativeAttrition |>\n    group_by(!!sym(var)) |>\n    count(Attrition) |>\n    group_modify(function(group, blah) calcLogLikelihoodData(select(group, Attrition, n))) |>\n    ungroup() |>\n    select(n) |>\n    sum() |>\n    (function(ll) (1-ll/nullModel$n)*100)()\n}\n#Test\n#calcPseudoRSquared(\"Age\")\n\n# Calculate Pseudo-R2 for all variables\npseudoRSquared = tibble(\"Variable\" = cols) |>\n  rowwise() |>\n  mutate(PseudoRSquared = calcPseudoRSquared(Variable))\n\nDisplay the pseudo-RSquared values for each variable, sorting by the highest to more easily identify top factors.\n\npseudoRSquared |>\n  ggplot(aes(x = PseudoRSquared, y = fct_reorder(Variable, PseudoRSquared))) +\n  geom_col() +\n  labs(x = \"Pseudo-RSquared\", y = NULL, title = \"Pseudo-RSquared Values for Each Variable\")\n\n\n\n\nBased on the analysis above, the top 3 factors influencing employee attrition are:\n\nJob Role\n\nStock Option Level\n\nOver Time\n\nThe next 3 factors following the top 3 are:\n\nTotal Working Years\n\nYears At Company\n\nMonthly Income"
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#learn-about-any-job-role-specific-trends-that-may-exist-in-the-data-set",
    "href": "SMU_DDS_CaseStudy2.html#learn-about-any-job-role-specific-trends-that-may-exist-in-the-data-set",
    "title": "Employee Attrition",
    "section": "2. Learn about any job role specific trends that may exist in the data set",
    "text": "2. Learn about any job role specific trends that may exist in the data set\nWe know that Job Role is our number one predictor of employee attrition. Looking closer at various reported job roles, it is clear that sales type positions see much larger amounts of turnover while managing and director type roles see very little.\n\nemployeeData |>\n  group_by(JobRole) |>\n  summarize(AttritionPct = 100*sum(ifelse(Attrition == \"No\", 0, 1))/n()) |>\n  ggplot(aes(x = AttritionPct, y = reorder(JobRole, AttritionPct))) +\n  geom_col() +\n  labs(title = \"Attrition Percentage Across Various Job Roles\", x = \"Attrition Percentage\", y = NULL)\n\n\n\n\nKnowing that stock options, working over time, and monthly income are also key indicators of attrition, we can use those to better understand why sales positions see higher attrition.\n\n# Looking at stock options\nemployeeDataBinnedWithQuantitativeAttrition |>\n  mutate(JobRole = factor(JobRole, levels = c(\"Sales Representative\", \"Human Resources\", \"Laboratory Technician\", \"Research Scientist\", \"Sales Executive\", \"Healthcare Representative\", \"Manager\", \"Manufacturing Director\", \"Research Director\"))) |>\n  group_by(JobRole) |>\n  count(StockOptionLevel) |>\n  ungroup() |>\n  ggplot(aes(x = JobRole, y = n, fill = StockOptionLevel)) +\n  geom_col(position = \"fill\") +\n  labs(title = \"Stock Option Breakdown Across Various Job Roles\", y = \"Fraction of Each Option Level\")\n\n\n\n# Looking at over time\nemployeeDataBinnedWithQuantitativeAttrition |>\n  mutate(JobRole = factor(JobRole, levels = c(\"Sales Representative\", \"Human Resources\", \"Laboratory Technician\", \"Research Scientist\", \"Sales Executive\", \"Healthcare Representative\", \"Manager\", \"Manufacturing Director\", \"Research Director\"))) |>\n  group_by(JobRole) |>\n  count(OverTime) |>\n  ungroup() |>\n  ggplot(aes(x = JobRole, y = n, fill = OverTime)) +\n  geom_col(position = \"fill\") +\n  labs(title = \"Over Time Breakdown Across Various Job Roles\", y = \"Fraction of Over Time\")\n\n\n\n# Looking at monthly income\nemployeeData |>\n  mutate(JobRole = factor(JobRole, levels = c(\"Sales Representative\", \"Human Resources\", \"Laboratory Technician\", \"Research Scientist\", \"Sales Executive\", \"Healthcare Representative\", \"Manager\", \"Manufacturing Director\", \"Research Director\"))) |>\n  ggplot(aes(x = JobRole, y = MonthlyIncome)) +\n  geom_boxplot() +\n  labs(title = \"Distributions of Monthly Income Across Various Job Roles\", y = \"Monthly Income\")\n\n\n\n\nAnalyzing Sales Representatives individually we see that they rank very highly in all of the categories that contribute the most to attrition. They generally have a proportionally smaller amount of stock options, they work slightly more over time than most other positions with the exception of research scientist, and they are the lowest paid."
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#provide-any-other-interesting-trends-and-observations-from-your-analysis",
    "href": "SMU_DDS_CaseStudy2.html#provide-any-other-interesting-trends-and-observations-from-your-analysis",
    "title": "Employee Attrition",
    "section": "3. Provide any other interesting trends and observations from your analysis",
    "text": "3. Provide any other interesting trends and observations from your analysis\nLet’s look at some of the quantitative variables to look for any interesting trends.\n\nemployeeData |>\n  select(Age, DistanceFromHome, Education, JobInvolvement, JobLevel, JobSatisfaction, MonthlyIncome, NumCompaniesWorked, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager) |>\n  cor() |>\n  round(2) |>\n  melt() |>\n  ggplot(aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile() +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(title = \"Correlation Matrix Between Various Numerical Indicators\")\n\n\n\n\nFrom the above correlation heat map, we can make the following observations:\n\nYears at company, years in current role, years since last promotion, and years with current manager are all closely related as they all generally give an indication of how long a particular employee has been at the current job\n\nHigh performance rating is correlated with higher salary hikes\n\nThere were no strong correlations for work life balance, which could mean there are other non-numerical indicators for work life balance, or it just depends on the individual"
  },
  {
    "objectID": "SMU_DDS_CaseStudy2.html#build-models-to-predict-attrition-and-salary",
    "href": "SMU_DDS_CaseStudy2.html#build-models-to-predict-attrition-and-salary",
    "title": "Employee Attrition",
    "section": "4. Build models to predict attrition and salary",
    "text": "4. Build models to predict attrition and salary\nTo predict attrition, a naive Bayes model was used with the goal of reaching the highest accuracy while preserving at least 60% for sensitivity and specificity. After cross-validating the model using a 75/25 train/test split and utilizing 16 of the top 30 predicting variables, the respective achievable accuracy, sensitivity, and specificity was found to be 80%, 84%, and 61%.\n\nemployeeDataForPrediction = employeeData |>\n  select(-ID, -EmployeeCount, -EmployeeNumber, -Over18, -StandardHours)\n\nset.seed(1234)\nevaluateModel = function() {\n  l = length(employeeDataForPrediction$Attrition)\n  indicies = sample(seq(1:l), round(0.75*l))\n  train = employeeDataForPrediction[indicies,]\n  test = employeeDataForPrediction[-indicies,]\n  \n  model = naiveBayes(Attrition ~ JobRole + StockOptionLevel + OverTime + TotalWorkingYears + YearsAtCompany + MonthlyIncome + JobLevel + Age + MaritalStatus + JobInvolvement + YearsInCurrentRole + NumCompaniesWorked + YearsWithCurrManager + PercentSalaryHike + WorkLifeBalance + JobSatisfaction, train)\n  predictions = predict(model, select(test, JobRole, StockOptionLevel, OverTime, TotalWorkingYears, YearsAtCompany, MonthlyIncome, JobLevel, Age, MaritalStatus, JobInvolvement, YearsInCurrentRole, NumCompaniesWorked, YearsWithCurrManager, PercentSalaryHike, WorkLifeBalance, JobSatisfaction))\n    \n  results = confusionMatrix(table(predictions, test$Attrition))\n  acc = as.numeric(results$overall[1])\n  sen = sensitivity(table(predictions, test$Attrition))\n  spc = specificity(table(predictions, test$Attrition))\n  \n  return(tibble(\"Accuracy\" = c(acc), \"Sensitivity\" = c(sen), \"Specificity \" = c(spc)))\n}\n\nresults = tibble(\"Accuracy\" = c(NULL), \"Sensitivity\" = c(NULL), \"Specificity \" = c(NULL))\nfor (variable in 1:200) {\n  results = rbind(results, evaluateModel())\n}\n\nresults |>\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Value\") |>\n  ggplot(aes(x = Variable, y = Value)) +\n  geom_boxplot() +\n  labs(title = \"Attrition Model Achievable Key Metrics\")\n\n\n\n\nPredicting attrition on the blind test set and then outputting to a csv file.\n\nblindTestData = read_csv(\"https://raw.githubusercontent.com/ayule89/SMU_DDS_CaseStudy2/main/Data/CaseStudy2CompSet%20No%20Attrition.csv\")\n\nmodel = naiveBayes(Attrition ~ JobRole + StockOptionLevel + OverTime + TotalWorkingYears + YearsAtCompany + MonthlyIncome + JobLevel + Age + MaritalStatus + JobInvolvement + YearsInCurrentRole + NumCompaniesWorked + YearsWithCurrManager + PercentSalaryHike + WorkLifeBalance + JobSatisfaction, employeeDataForPrediction)\n\npredictions = predict(model, select(blindTestData, JobRole, StockOptionLevel, OverTime, TotalWorkingYears, YearsAtCompany, MonthlyIncome, JobLevel, Age, MaritalStatus, JobInvolvement, YearsInCurrentRole, NumCompaniesWorked, YearsWithCurrManager, PercentSalaryHike, WorkLifeBalance, JobSatisfaction))\n\nblindTestResults = tibble(\"ID\" = blindTestData$ID, \"Attrition\" = predictions)\nwrite_csv(blindTestResults,\"/Users/andrewyule/Dropbox/Personal/SMU MSDS/03 - Doing Data Science/SMU_DDS_CaseStudy2/Case 2 Predictions Yule Attrition.csv\")\n\nTo predict salaries, a linear regression model was used. The goal was to achieve a high R Squared value and minimal root mean squared error (RMSE), while keeping the number of predictors as low as possible. The variables, Age, Job Role, Years At Company, and Stock Options were found to be good in predicting salary and lead to a model with an RSquared of around 0.85 and a RMSE of $1,850.\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted)^2))\n}\n\nset.seed(1234)\nevaluateModel = function() {\n  l = length(employeeData$MonthlyIncome)\n  indicies = sample(seq(1:l), round(0.75*l))\n  train = employeeData[indicies,]\n  test = employeeData[-indicies,]\n  \n  model = lm(MonthlyIncome ~ Age + JobRole + YearsAtCompany + StockOptionLevel + PerformanceRating, data = train)\n  predictions = predict(model, test)\n  tibble(\"RSquared\" = summary(model)$r.squared, \"RMSE\" = rmse(test$MonthlyIncome, predictions))\n}\n\nresults = tibble(\"RSquared\" = c(NULL), \"RMSE\" = c(NULL))\nfor (variable in 1:200) {\n  results = rbind(results, evaluateModel())\n}\n\nresults |>\n  pivot_longer(cols = everything(), names_to = \"Variable\", values_to = \"Value\") |>\n  ggplot(aes(x = Variable, y = Value)) +\n  geom_boxplot() +\n  facet_wrap(~Variable, scales = \"free\")\n\n\n\n\nPredicting attrition on the blind test set and then outputting to a csv file.\n\nblindTestData = read_csv(\"https://raw.githubusercontent.com/ayule89/SMU_DDS_CaseStudy2/main/Data/CaseStudy2CompSet%20No%20Salary.csv\")\n\nRows: 300 Columns: 35\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\ndbl (26): ID, Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Em...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmodel = lm(MonthlyIncome ~ Age + JobRole + YearsAtCompany + StockOptionLevel + PerformanceRating, data = employeeData)\npredictions = predict(model, blindTestData)\n\nblindTestResults = tibble(\"ID\" = blindTestData$ID, \"MonthlyIncome\" = predictions)\nwrite_csv(blindTestResults,\"/Users/andrewyule/Dropbox/Personal/SMU MSDS/03 - Doing Data Science/SMU_DDS_CaseStudy2/Case 2 Predictions Yule Salary.csv\")"
  },
  {
    "objectID": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html",
    "href": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html",
    "title": "Budweiser - Beer and Brewery Analysis",
    "section": "",
    "text": "library(tidyverse)\nlibrary(maps)\nlibrary(gridExtra)\nlibrary(class)\nlibrary(caret)\nlibrary(e1071)\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html#data-import",
    "href": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html#data-import",
    "title": "Budweiser - Beer and Brewery Analysis",
    "section": "Data import",
    "text": "Data import\n\nImport the beers and breweries data sets and do minor cleanup\n\nbeers = read_csv(\"/Users/andrewyule/Dropbox/Personal/SMU MSDS/03 - Doing Data Science/SMU_DDS_CaseStudy1/Data/Beers.csv\")\nbreweries = read_csv(\"/Users/andrewyule/Dropbox/Personal/SMU MSDS/03 - Doing Data Science/SMU_DDS_CaseStudy1/Data/Breweries.csv\")\n\n# Change ABV from fraction to percent\nbeers = beers |>\n  mutate(ABV = ABV * 100)"
  },
  {
    "objectID": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html#analysis-questions",
    "href": "Budweiser - Beer and Brewery Analysis - Andrew Yule.html#analysis-questions",
    "title": "Budweiser - Beer and Brewery Analysis",
    "section": "Analysis questions",
    "text": "Analysis questions\n\n1. How many breweries are present in each state?\nIn the data, Colorado was found to have the most number of breweries from the data set (47), followed by California (39), Michigan (32), Oregon (29), and Texas (28). The lowest counted number of breweries was 1, which was found in Washington DC, North Dakota, South Dakota and West Virigina.\n\nbreweryCount = breweries |> count(State)\nstateAbbreviations = tibble(abb = state.abb, name = tolower(state.name))\nbreweryCount = left_join(breweryCount, stateAbbreviations, by = c(\"State\" = \"abb\"))\n\nstates = as_tibble(map_data(\"state\"))\nbreweryCount = left_join(states, breweryCount, by = c(\"region\" = \"name\"))\nggplot(breweryCount, aes(x = long, y = lat, group = group))+\n  geom_polygon(aes(fill = n))+\n  geom_path() + \n  scale_fill_gradientn(colours = rev(heat.colors(10)), na.value = \"grey90\") +\n  coord_map() +\n  labs(x = NULL, y = NULL) +\n  theme_void()\n\n\n\n\nWhat percentage of breweries do the top 5 states make up?\n\nsum(arrange(count(breweries, State), desc(n))[1:5, 2]) / sum(count(breweries, State)[, 2])\n\n[1] 0.3136201\n\n\n\n\n2. Merge the beers and breweries data sets to create a single data set containing both beer information as well as brewery\nThe two datasets (beers and breweries) were merged using a left_join across the columns “Brewery_ID” (for beers) and “Brew_ID” (for breweries). The left join ensures that we only insert brewery information where a matching beer is found.\n\nbeers = left_join(beers, breweries, by = c(\"Brewery_id\" = \"Brew_ID\")) |>\n  rename(Beer = Name.x) |>\n  rename(Brewery = Name.y)\n\nbeers |> head(n = 6)\n\n# A tibble: 6 × 10\n  Beer           Beer_ID   ABV   IBU Brewery_id Style Ounces Brewery City  State\n  <chr>            <dbl> <dbl> <dbl>      <dbl> <chr>  <dbl> <chr>   <chr> <chr>\n1 Pub Beer          1436   5      NA        409 Amer…     12 10 Bar… Bend  OR   \n2 Devil's Cup       2265   6.6    NA        178 Amer…     12 18th S… Gary  IN   \n3 Rise of the P…    2264   7.1    NA        178 Amer…     12 18th S… Gary  IN   \n4 Sinister          2263   9      NA        178 Amer…     12 18th S… Gary  IN   \n5 Sex and Candy     2262   7.5    NA        178 Amer…     12 18th S… Gary  IN   \n6 Black Exodus      2261   7.7    NA        178 Oatm…     12 18th S… Gary  IN   \n\nbeers |> tail(n = 6)\n\n# A tibble: 6 × 10\n  Beer           Beer_ID   ABV   IBU Brewery_id Style Ounces Brewery City  State\n  <chr>            <dbl> <dbl> <dbl>      <dbl> <chr>  <dbl> <chr>   <chr> <chr>\n1 Rocky Mountai…    1035   7.5    NA        425 Amer…     12 Wynkoo… Denv… CO   \n2 Belgorado          928   6.7    45        425 Belg…     12 Wynkoo… Denv… CO   \n3 Rail Yard Ale      807   5.2    NA        425 Amer…     12 Wynkoo… Denv… CO   \n4 B3K Black Lag…     620   5.5    NA        425 Schw…     12 Wynkoo… Denv… CO   \n5 Silverback Pa…     145   5.5    40        425 Amer…     12 Wynkoo… Denv… CO   \n6 Rail Yard Ale…      84   5.2    NA        425 Amer…     12 Wynkoo… Denv… CO   \n\n\n\n\n3. Address the missing values in each column.\nThe only columns from the data set containing missing entries are ABV (alcohol by volume), IBU (international bitterness units), and Style. The highest number of missing values was found for the IBU variable (42% of values are missing). The high amount of missing values for IBU should be considered in any further analysis involving this variable.\n\n(colSums(is.na(beers))/dim(beers)[1])*100\n\n      Beer    Beer_ID        ABV        IBU Brewery_id      Style     Ounces \n 0.0000000  0.0000000  2.5726141 41.7012448  0.0000000  0.2074689  0.0000000 \n   Brewery       City      State \n 0.0000000  0.0000000  0.0000000 \n\n\n\n\n4. / 5. Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare. Which state has the maximum alcoholic (ABV) beer? Which state has the most bitter (IBU) beer?\nThe median alcohol by volume (ABV) across the states varies between 0.04 (Utah) and 0.0625 (Washington DC & Kentucky). The median international bitterness unit (IBU) varies between 19 (Wisconsin) and 61 (Maine).\n\nmedianABV = beers |>\n  filter(!is.na(ABV)) |>\n  group_by(State) |>\n  summarize(ABV = median(ABV))\n\nmedianIBU = beers |>\n  filter(!is.na(IBU)) |>\n  group_by(State) |>\n  summarize(IBU = median(IBU))\n\np1 = medianABV |>\n  ggplot(aes(x = reorder(State, ABV), y = ABV)) +\n  geom_col(fill = \"#C80F2D\") +\n  labs(y = \"Median ABV\", x = NULL, title = \"Median Alcohol by Volume (ABV) Across States\") +\n  theme(text=element_text(size=18))\n\np2 = medianIBU |>\n  ggplot(aes(x = reorder(State, IBU), y = IBU)) +\n  geom_col(fill = \"#C80F2D\") +\n  labs(y = \"Median IBU\", x = NULL, title = \"Median International Bitterness Unit (IBU) Across States\") +\n  theme(text=element_text(size=18))\n\ngrid.arrange(p1, p2)\n\n\n\nbeers |>\n  filter(!is.na(ABV)) |>\n  group_by(State) |>\n  summarize(MedianABV = median(ABV)) |>\n  left_join(stateAbbreviations, by = c(\"State\" = \"abb\")) |>\n  left_join(states, y = _, by = c(\"region\" = \"name\")) |>\n  ggplot(aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = MedianABV)) +\n  geom_path() + \n  scale_fill_gradientn(colours = rev(heat.colors(10)), na.value = \"grey90\") +\n  coord_map() +\n  labs(x = NULL, y = NULL) +\n  theme_void()\n\n\n\nbeers |>\n  filter(!is.na(IBU)) |>\n  group_by(State) |>\n  summarize(MedianIBU = median(IBU)) |>\n  left_join(stateAbbreviations, by = c(\"State\" = \"abb\")) |>\n  left_join(states, y = _, by = c(\"region\" = \"name\")) |>\n  ggplot(aes(x = long, y = lat, group = group)) + \n  geom_polygon(aes(fill = MedianIBU)) +\n  geom_path() + \n  scale_fill_gradientn(colours = rev(heat.colors(10)), na.value = \"grey90\") +\n  coord_map() +\n  labs(x = NULL, y = NULL) +\n  theme_void()\n\n\n\n\nThe absolute maximum ABV and IBU for any state\n\nslice_max(beers, ABV, n = 1)\n\n# A tibble: 1 × 10\n  Beer           Beer_ID   ABV   IBU Brewery_id Style Ounces Brewery City  State\n  <chr>            <dbl> <dbl> <dbl>      <dbl> <chr>  <dbl> <chr>   <chr> <chr>\n1 Lee Hill Seri…    2565  12.8    NA         52 Quad…   19.2 Upslop… Boul… CO   \n\nslice_max(beers, IBU, n = 1)\n\n# A tibble: 1 × 10\n  Beer           Beer_ID   ABV   IBU Brewery_id Style Ounces Brewery City  State\n  <chr>            <dbl> <dbl> <dbl>      <dbl> <chr>  <dbl> <chr>   <chr> <chr>\n1 Bitter Bitch …     980   8.2   138        375 Amer…     12 Astori… Asto… OR   \n\n\n\n\n6. Comment on the summary statistics and distribution of the ABV variable.\nOverall, the average ABV is around 0.059, with a minimum and maximum reported value of 0.001 and 0.128 respectively. The distribution of ABV is right skewed, meaning there are a few beers with very high alcohol contents increasing the mean ABV compared to the median ABV.\n\nbeers |> \n  select(ABV) |>\n  summary()\n\n      ABV        \n Min.   : 0.100  \n 1st Qu.: 5.000  \n Median : 5.600  \n Mean   : 5.977  \n 3rd Qu.: 6.700  \n Max.   :12.800  \n NA's   :62      \n\nbeers |> \n  ggplot(aes(x = ABV)) +\n  geom_histogram(fill = \"#C80F2D\") +\n  labs(x = \"ABV (%)\", y = \"Frequency\", title = \"Distribution of ABV Values Across Beers\") +\n  theme(text=element_text(size=18))\n\n\n\n\n\n\n7. Is there an apparent relationship between the bitterness of the beer and its alcoholic content? Draw a scatter plot. Make your best judgment of a relationship and EXPLAIN your answer.\nThe scatterplot between IBU and ABV shows that there is a positive correlation (0.67). This indicates that as IBU increases, it is expected that ABV will increase as well.\n\ncor(beers[!is.na(beers$ABV) & !is.na(beers$IBU),]$IBU, beers[!is.na(beers$ABV) & !is.na(beers$IBU),]$ABV)\n\n[1] 0.6706215\n\nbeers |>\n  ggplot(aes(x = IBU, y = ABV)) +\n  geom_point(color = \"#C80F2D\") +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"IBU\", y = \"ABV (%)\", title = \"Relationship between IBU and ABV\") +\n  theme(text=element_text(size=18))\n\n\n\n\n\n\n8. Investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale.\nVisual evidence shows that there is distinct difference between IBU and ABV values of IPA’s compared with Ales. In general, IPA have both higher IBU and ABV.\n\nipa = beers |>\n  filter(!is.na(ABV) & !is.na(IBU)) |>\n  filter(str_detect(Style, \"(?:IPA|India.*Pale.*Ale)\")) |>\n  select(ABV, IBU) |>\n  mutate(Style = \"IPA\")\n\nales = beers |>\n  filter(!is.na(ABV) & !is.na(IBU)) |>\n  anti_join(ipa) |>\n  filter(str_detect(Style, \"Ale\")) |>\n  select(ABV, IBU) |>\n  mutate(Style = \"Ale\")\n\nipaAndAles = full_join(ipa, ales)\n\n# Visually show the relationship between IBU and ABV broken down between IPA's and Ales\nipaAndAles |>\n  ggplot(aes(x = IBU, y = ABV, color = Style)) +\n  geom_point(size = 3) +\n  labs(x = \"IBU\", y = \"ABV (%)\", title = \"Relationship Between IBU and ABV\", subtitle = \"Broken Down By IPA's and Ale's\") +\n  theme(text=element_text(size=18))\n\n\n\n\nBased on the visual evidence above, classifying whether a beer is either an IPA or Ale should be possible with high accuracy. To test this theory, a k nearest neighbors classification (kNN) algorithm was used. After cross validating using a 80/20 train/test split, an optimal k value of 5 was found, which yielded an overall classification accuracy of ~85% confirming that it is possible to distinguish between an IPA and an Ale based on the beers IBU and ABV.\n\nset.seed(1234)\n# Create a function to evaluate classification accuracy using an 80/20 split with varying values of k\nevaluateKNN = function(k = 1){\n  split = 0.8\n  indicies = sample(1:dim(ipaAndAles)[1], ceiling(dim(ipaAndAles)[1] * split))\n  train = ipaAndAles[indicies, ]\n  test = ipaAndAles[-indicies, ]\n  results = knn(train[, c(1, 2)], test[, c(1, 2)], ifelse(train[, 3] == \"IPA\", 0, 1), k = k)\n  confusionMatrix(as_factor(c(ifelse(test[, 3] == \"IPA\", 0, 1))), results)$overall[1]\n}\n\noptimalK = crossing(set = 1:100, k = 1:100)\noptimalK = optimalK |>\n  rowwise() |>\n  mutate(Accuracy = evaluateKNN(k))\n\n# Plot the results of the cross validation showing possible achievable accuracy\noptimalK |>\n  group_by(k) |>\n  summarize(Accuracy = mean(Accuracy)) |>\n  ggplot(aes(x = k, y = Accuracy)) + \n  geom_point(color = \"#C80F2D\") +\n  geom_path(color = \"#C80F2D\") +\n  labs(x = \"k\", y = \"Average Accuracy\", title = \"Classification Accuracy of IPA's and Ales\", subtitle = \"kNN Classification Algorithm Used With Varying k Values\") +\n  theme(text=element_text(size=18))\n\n\n\n\n\n\n9. Which states may be under represented by the number of breweries and would thus benefit from Budweiser investment?\nA visual of the population of each state ploted on a log-log scale with the number of breweries along with a linear line of best fit allows for a distinction between states that potentially have more saturated number of breweries as compared to less. From this visual, it can be seen that states like Georgia, New Jersey, Tennessee, Alabama, and South Carolina are all potentially under represented in their brewery count while still having high enough populations to make investment in more breweries economical for Budweiser.\n\nstatePopulations = read_csv(\"/Users/andrewyule/Dropbox/Personal/SMU MSDS/03 - Doing Data Science/SMU_DDS_CaseStudy1/Data/State_Populations.csv\")\n\nstatesWithBreweriesAndPopulations = left_join(breweries, statePopulations, by = \"State\") |>\n  group_by(State) |>\n  summarize(Breweries = n(), Population = Population) |>\n  filter(!duplicated(State, Breweries, Population))\n\nstatesWithBreweriesAndPopulations |>\n  ggplot(aes(x = Population, y = Breweries)) +\n  geom_point(size = 3, color = \"#C80F2D\") +\n  geom_text(aes(label = State), nudge_y = 0.05, color = \"#C80F2D\", size = 5) +\n  geom_smooth(method = \"lm\") +\n  #scale_x_log10(labels = ~ format(.x, scientific = FALSE)) +\n  scale_x_log10(labels = scales::comma) +\n  scale_y_log10() +\n  labs(x = \"State Population\", y = \"Number of Breweries\", title = \"Brewery Count with State Population\") +\n  theme(text=element_text(size=18))"
  }
]